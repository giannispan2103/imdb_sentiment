{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ProjectedAttentionTextRNN, TextRNN, AttentionTextRNN, TextSummaryRNN, ProjectedTextRNN\n",
    "from preprocess import generate_data\n",
    "from utils import train\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_text():\n",
    "    data = generate_data(emb_size=100, max_len=500)\n",
    "    emb_matrix = data['emb_matrix']\n",
    "    train_batches = data['train_batches']\n",
    "    test_batches = data['test_batches']\n",
    "    model = ProjectedAttentionTextRNN(emb_matrix, stacked_layers=1).cuda()\n",
    "    optimizer = Adam(model.params, 0.001)\n",
    "    criterion = BCEWithLogitsLoss()\n",
    "    train(model, train_batches, test_batches, optimizer, criterion, 50, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with: train - pos\n",
      "finished with: train - neg\n",
      "finished with: test - pos\n",
      "finished with: test - neg\n",
      "reviews for training: 27142\n",
      "reviews for testing: 25000\n",
      "tokens found in training data set: 263522\n",
      "tokens with frequency >= 1: 263522\n",
      "embedded tokens with frequency >= 1: 50905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Documents\\Python Scripts\\imdb_sentiment\\modules.py:193: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  att_weights = self.at_softmax(att_sc_dist).unsqueeze(2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no embedding for:  *$*PAD*$*\n",
      "no embedding for:  *$*UNK*$*\n",
      "epoch 1, f1: 80.298 accuracy: 79.724 auc: 87.815. Time: 0 minutes, 14 seconds\n",
      "epoch 2, f1: 83.770 accuracy: 83.564 auc: 91.532. Time: 0 minutes, 14 seconds\n",
      "best epoch so far\n",
      "epoch 3, f1: 84.512 accuracy: 84.256 auc: 92.223. Time: 0 minutes, 15 seconds\n",
      "best epoch so far\n",
      "epoch 4, f1: 85.257 accuracy: 85.076 auc: 92.750. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 5, f1: 85.585 accuracy: 85.376 auc: 93.083. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 6, f1: 85.826 accuracy: 85.688 auc: 93.316. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 7, f1: 86.039 accuracy: 85.764 auc: 93.449. Time: 0 minutes, 19 seconds\n",
      "best epoch so far\n",
      "epoch 8, f1: 86.318 accuracy: 86.104 auc: 93.640. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 9, f1: 86.397 accuracy: 86.268 auc: 93.744. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 10, f1: 86.754 accuracy: 86.344 auc: 93.813. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 11, f1: 86.794 accuracy: 86.572 auc: 93.967. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 12, f1: 87.182 accuracy: 86.968 auc: 94.164. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 13, f1: 87.136 accuracy: 86.964 auc: 94.215. Time: 0 minutes, 18 seconds\n",
      "epoch 14, f1: 86.968 accuracy: 86.780 auc: 94.089. Time: 0 minutes, 18 seconds\n",
      "epoch 15, f1: 87.399 accuracy: 87.204 auc: 94.394. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 16, f1: 87.502 accuracy: 87.240 auc: 94.421. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 17, f1: 87.402 accuracy: 87.188 auc: 94.367. Time: 0 minutes, 18 seconds\n",
      "epoch 18, f1: 87.500 accuracy: 87.400 auc: 94.539. Time: 0 minutes, 18 seconds\n",
      "epoch 19, f1: 87.661 accuracy: 87.364 auc: 94.428. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 20, f1: 87.752 accuracy: 87.464 auc: 94.567. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 21, f1: 87.775 accuracy: 87.628 auc: 94.626. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 22, f1: 87.782 accuracy: 87.684 auc: 94.620. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 23, f1: 87.716 accuracy: 87.576 auc: 94.611. Time: 0 minutes, 18 seconds\n",
      "epoch 24, f1: 87.799 accuracy: 87.612 auc: 94.570. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 25, f1: 87.859 accuracy: 87.704 auc: 94.636. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 26, f1: 87.958 accuracy: 87.872 auc: 94.682. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 27, f1: 87.848 accuracy: 87.484 auc: 94.438. Time: 0 minutes, 18 seconds\n",
      "epoch 28, f1: 88.041 accuracy: 87.992 auc: 94.699. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 29, f1: 87.915 accuracy: 87.928 auc: 94.588. Time: 0 minutes, 18 seconds\n",
      "epoch 30, f1: 88.073 accuracy: 87.972 auc: 94.661. Time: 0 minutes, 18 seconds\n",
      "best epoch so far\n",
      "epoch 31, f1: 88.053 accuracy: 87.860 auc: 94.578. Time: 0 minutes, 18 seconds\n",
      "epoch 32, f1: 87.911 accuracy: 87.808 auc: 94.438. Time: 0 minutes, 18 seconds\n",
      "epoch 33, f1: 87.959 accuracy: 87.876 auc: 94.498. Time: 0 minutes, 18 seconds\n",
      "epoch 34, f1: 87.947 accuracy: 87.880 auc: 94.655. Time: 0 minutes, 18 seconds\n",
      "epoch 35, f1: 88.063 accuracy: 87.976 auc: 94.507. Time: 0 minutes, 18 seconds\n"
     ]
    }
   ],
   "source": [
    "run_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
